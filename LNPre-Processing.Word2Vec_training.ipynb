{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing, Word2Vec Model Training and Perfomance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Part 1: Pre-processing: This code takes in a text file with the text data (specifically written for NYT Text Data in txt format from Lexis Nexis) \n",
    "\n",
    "* Part 2: Train a Word2Vec model on this text data\n",
    "\n",
    "* Part 3: Get accuracy scores on a trained Word2Vec model based on the Google Analogy Test\n",
    "\n",
    "* Part 4: Quick Exploration of a Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggestions along the way for clean dataset to just jump into Part 2 (training), or a pre-trained model to jump into model exploration (Part 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    \n",
    "from nltk.corpus import stopwords\n",
    "import cython #ENSURE cython package is installed on computer/canopy\n",
    "import sklearn.cluster\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import phrases \n",
    "np.set_printoptions(threshold=np.inf) #set to print full output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#upload text file with sentences object, which includes only sentences drawn the text data you want to learn the Word2Vec model on\n",
    "\n",
    "fname= \"dataALLYEARS.txt\"\n",
    "lnraw=open(fname, encoding=\"utf8\").read() #read the file, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lnraw2= lnraw[3:] #this is for my data, get rid of extra stuff at beginning of each lexis nexis file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO MAKE SURE TEXT FILE LOADED CORRECTLY:\n",
    "len(lnraw2)\n",
    "#lnraw2= lnraw2[:500000] #for testing code, shorten number characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace these punctuations with periods, to show they are all delimiters for sentences, #double check this is working on a variety of punctuation. also recall goop at end of each article, how will this affect it?\n",
    "lnrawtrial2 = re.sub(';?:!\"', '.', lnraw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lnrawtrial2 = re.sub(r'[^\\w\\s.]',' ', lnrawtrial2)  #remove all punctuation except periods, and replace with spaces\n",
    "lnrawtrial2= lnrawtrial2.lower()  #make all letters lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Stopwords? (Optional, not done for paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove stopwords ....sometimes better not to do this, its subjective. See: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors\n",
    "lnrawtrial3= lnrawtrial2.split()\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "lnrawtrial4 = ' '.join([word for word in lnrawtrial3 if word not in cachedStopWords])\n",
    "\n",
    "#note that now this is again formatted so each element is a character, so split again:\n",
    "lnrawtrial5= lnrawtrial4.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not removing stopwords, skip right to here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if not removing stopwords/rare words do this\n",
    "lnrawtrial7=lnrawtrial2 \n",
    "\n",
    "#NOW, regardless of stopwords/rarewords:\n",
    "\n",
    "#last, strsplitr on periods to turn into a \"sentences\" object. obj= [[\"w1sent1\", \"w2sent1\"], [\"w1sent2\", \"w2sent2\", \"w3sent2\"]]\n",
    "lnrawtrial8= lnrawtrial7.split('.') #there are also other ways to split into sentences, such also splitting on ! ? ; :\n",
    "\n",
    "lnrawtrial9 = [None] * len(lnrawtrial8)\n",
    "for sent in range(1,len(lnrawtrial8)):\n",
    "    lnrawtrial9[sent]=list(filter(None, lnrawtrial8[sent].split(\" \"))) #filter gets out '' after doing the sent split into words, in py 3 need to wrap with list(), but not in py 2\n",
    "\n",
    "lnrawtrial10=lnrawtrial9[1:] #stubborn space at beginning of my data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save your cleaned, tokenized text data for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('alldat_NYT_tokenized.txt','a')\n",
    "f.writelines([\"%s\\n\" % item  for item in lnrawtrial10]) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save some space if you ran pre-prepocessing code\n",
    "del lnraw\n",
    "del lnrawtrial2\n",
    "#del lnrawtrial3\n",
    "#del lnrawtrial4\n",
    "#del lnrawtrial5\n",
    "#del lnrawtrial6\n",
    "del lnrawtrial7\n",
    "del lnrawtrial8\n",
    "del lnrawtrial9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Train a Word2Vec Model, Explore Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't have a dataset? Gensim has a few free suggestions, such as the Text8 Corpus:\n",
    "* https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Text8Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's a possible baseline model to get to know arguments:\n",
    "\n",
    "Word2Vec in Gensim Documentation/Tutorial: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notes on a few hyperparameters:\n",
    "* num_features = word vector dimensionality. usually ranges 50-500 with gains in model performance diminishing after 300. \n",
    "* min_word_count = minimum word count. Any word that does not occur at least this many times across all documents is ignored, I use 40 after seeing this doen on Kaggle code. \n",
    "* num_workers = number of threads to run in parallel, I set to 4. This SERIOUSLY speeds up training, but to use make sure Cython is installed. \n",
    "* context = context window size, 10 in kaggle, larger window is more about topical similarity, smaller is semantic similiarity\n",
    "* downsampling = downsample setting for frequent words. \n",
    "* seedie= can set a seed for reproducibility, note you also cannot use multiple workers if you want a fully reproducible model.\n",
    "* sg = learning archietecture: skip-gram (1) or CBOW (0)\n",
    "* hs= training algorithm to speed up computations: hierarchical softmax (1) or negative sampling (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Models with Four Combos of Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying out the four possible combinations on training data, I found that Model A performs the best on my training data and used this as my final\n",
    "model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_transformer = phrases.Phrases(lnrawtrial10) #lnrawtrial10 is my \"sentences\" object with the cleaned text data. do this first before training model if you want to allow for bigrams in data (two word expressions, rather than only one word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alina Arseniev\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\") \n",
    "modelA_ALLYEARS= word2vec.Word2Vec(bigram_transformer[lnrawtrial10], workers=4, sg=0,\n",
    "                          size=100, min_count=40,\n",
    "                          window=5, sample=1e-3)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "modelB_ALLYEARS= Word2Vec(bigram_transformer[lnrawtrial10], workers=4, sg=1, \n",
    "                          size=500, min_count=40,\n",
    "                          window=10, sample=1e-3)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "modelC_ALLYEARS= Word2Vec(bigram_transformer[lnrawtrial10],  workers=4, sg=0, hs=1,\n",
    "                          size=500, min_count=40, \n",
    "                          window=10, sample=1e-3)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "modelD_ALLYEARS= Word2Vec(bigram_transformer[lnrawtrial10], workers=4, sg=1, hs=1,\n",
    "                          size=500, min_count=40,\n",
    "                          window=10, sample=1e-3)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelA_ALLYEARS2.init_sims(replace=True) #Precompute L2-normalized vectors. If replace is set to TRUE, forget the original vectors and only keep the normalized ones = saves lots of memory, but can't continue to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelA_ALLYEARS2.save(\"your_model_name_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Word2Vec Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload a trained model you want to evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "currentmodel=  Word2Vec.load(\"modelA_ALLYEARS_500dim_10CW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't have a model? Use a pretrained Word2Vec Model from Google, trained on Google News\n",
    "* Read and download here: https://code.google.com/archive/p/word2vec/\n",
    "* File is called \"GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy=currentmodel.accuracy('questions_words_pasted.txt') #FOR THIS TO WORK, NEED TO HAVE THIS TXT FILE IN WORKING DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "world_capitals1= accuracy[0] #seems same as world_capitals1 and 2 and 3??\n",
    "world_capitals2= accuracy[1] #seems same as world_capitals1 and 2 and 3??\n",
    "money= accuracy[2]\n",
    "US_capitals= accuracy[3]\n",
    "family= accuracy[4]\n",
    "adj_to_adverbs= accuracy[5]\n",
    "opposites= accuracy[6]\n",
    "comparative= accuracy[7]\n",
    "superlative= accuracy[8]\n",
    "present_particple= accuracy[9]\n",
    "nationality= accuracy[10]\n",
    "past_tense= accuracy[11]\n",
    "plural = accuracy[12]\n",
    "plural_verbs= accuracy[13]\n",
    "word_capitals3= accuracy[14] #seems same as world_capitals1 and 2 and 3??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5684210526315789\n",
      "0.4547244094488189\n",
      "0.1\n",
      "0.1790794979079498\n",
      "0.8973684210526316\n",
      "0.1431451612903226\n",
      "0.29210526315789476\n",
      "0.8536036036036037\n",
      "0.7210144927536232\n",
      "0.7149425287356321\n",
      "0.5718206770356816\n",
      "0.6518218623481782\n",
      "0.6857142857142857\n",
      "0.7321937321937322\n",
      "0.5711067051189618\n"
     ]
    }
   ],
   "source": [
    "for i in accuracy:\n",
    "    sum_corr = len(i['correct'])\n",
    "    sum_incorr = len(i['incorrect'])\n",
    "    total = sum_corr + sum_incorr\n",
    "    print(float(sum_corr)/(total))\n",
    "\n",
    "#print('Total sentences: {}, Correct: {:.2f}%, Incorrect: {:.2f}%'.format(total, percent(sum_corr), percent(sum_incorr)))\n",
    "\n",
    "#print sum_corr[1]\n",
    "#print sum_incorr[1]\n",
    "#print total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Word2Vec Model Exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload a trained model you want to explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "currentmodel=  Word2Vec.load(\"modelA_ALLYEARS_500dim_10CW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't have a model? Use a pretrained Word2Vec Model from Google, trained on Google News\n",
    "* Read and download here: https://code.google.com/archive/p/word2vec/\n",
    "* File is called \"GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities #calc all similarities at once, from http://radimrehurek.com/gensim/tut3.html\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currentmodel['woman'] #this gives the word-vector for 'woman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.754645441633\n"
     ]
    }
   ],
   "source": [
    "result = 1 - spatial.distance.cosine(currentmodel['computer'], currentmodel['software']) #calculate cosine similarity more manually, 2x checked against model.most_similar and results match\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('obese', 0.8001347780227661),\n",
       " ('underweight', 0.6493887901306152),\n",
       " ('normal_weight', 0.5515424609184265),\n",
       " ('being_overweight', 0.5171246528625488),\n",
       " ('anorexic', 0.5130723714828491)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentmodel.most_similar('overweight', topn=5) #asks which words are closest to overweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lab_tests', 0.24826177954673767),\n",
       " ('sterilization', 0.2312188297510147),\n",
       " ('endocrinologist', 0.2287030965089798),\n",
       " ('bouvia', 0.22845697402954102),\n",
       " ('auschwitz', 0.2241036295890808),\n",
       " ('legal_rights', 0.22197261452674866),\n",
       " ('antidepressant_drugs', 0.21886323392391205),\n",
       " ('pliny', 0.21702751517295837),\n",
       " ('medically', 0.21255454421043396),\n",
       " ('were_randomly', 0.206563338637352)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentmodel.most_similar(negative=['big']) #asks which words are FARTHEST from big, doesn't work so well. Shows how distance as can break down as a meaninful measure in this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hypertension', 0.5848986506462097), ('gout', 0.5804318785667419), ('neurological', 0.5797110795974731), ('congenital', 0.5704867839813232), ('diabetes', 0.5627163648605347), ('atherosclerosis', 0.5601633191108704), ('infectious', 0.5572311878204346), ('retardation', 0.5475963950157166), ('cognition', 0.5436995029449463), ('gastrointestinal', 0.5427045226097107)]\n",
      "break\n",
      "[('diabetes', 0.6686073541641235), ('pregnancy', 0.6431320905685425), ('estrogen', 0.6330111622810364), ('childbirth', 0.6287267804145813), ('asthma', 0.6252803802490234), ('alcoholism', 0.6236932277679443), ('anorexia', 0.619240403175354), ('reproductive', 0.6185950040817261), ('syndrome', 0.6179408431053162), ('diseases', 0.615617573261261)]\n"
     ]
    }
   ],
   "source": [
    "print currrentmodel.most_similar(positive=['man', 'obesity'], negative=['woman']) #man:king as woman:_?___\n",
    "print \" \"\n",
    "print currentmodel.most_similar(positive=['woman', 'obesity'], negative=['man']) #man:king as woman:_?___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noodle'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentmodel.doesnt_match(\"noodle chicken turkey beef\".split()) #A TRICK! My models seems to do pretty well when I try to trick them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Raw Word/Word-Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could do k-means on this (but computationally expensive), explore the model diemsnions learned, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordvecs= currentmodel.syn0 #if run, this outputs all word-vectors en masse http://rare-technologies.com/word2vec-tutorial/, but doesn't have name of word attached to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = open('test.csv', 'wb') #writes a csv with a row for each word-vector, total 17,855 rows. But, this data doesn't include word represented by each word-vector. \n",
    "a = csv.writer(b)\n",
    "'''\n",
    "testdata = [['Me', 'You'],\\\n",
    "        ['293', '219'],\\\n",
    "        ['54', '13']]'''\n",
    "a.writerows(wordvecs)\n",
    "b.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17855"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(currentmodel.vocab) #this is the length of the list of all words in the Word2Vec model's vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
