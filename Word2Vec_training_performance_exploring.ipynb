{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model Training, Perfomance Evaluation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Part 1: Train a Word2Vec model on this text data\n",
    "\n",
    "* Part 2: Get accuracy scores on a trained Word2Vec model based on the Google Analogy Test\n",
    "\n",
    "* Part 3: Quick Exploration of a Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggestions along the way for clean dataset to use for into Part 1 (training), or a pre-trained model to jump into model exploration (Part 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cython #ENSURE cython package is installed on computer/canopy\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import phrases \n",
    "from gensim import corpora, models, similarities #calc all similarities at once, from http://radimrehurek.com/gensim/tut3.html\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "import csv\n",
    "np.set_printoptions(threshold=np.inf) #set to print full output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Train a Word2Vec Model, Explore Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't have a dataset? Gensim has a few free suggestions, such as the Text8 Corpus:\n",
    "* https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Text8Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's a possible baseline model to get to know arguments:\n",
    "\n",
    "Word2Vec in Gensim Documentation/Tutorial: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notes on a few hyperparameters:\n",
    "* num_features = word vector dimensionality. usually ranges 50-500 with gains in model performance diminishing after 300. \n",
    "* min_word_count = minimum word count. Any word that does not occur at least this many times across all documents is ignored, I use 40 after seeing this doen on Kaggle code. \n",
    "* num_workers = number of threads to run in parallel, I set to 4. This SERIOUSLY speeds up training, but to use make sure Cython is installed. \n",
    "* context = context window size, 10 in kaggle, larger window is more about topical similarity, smaller is semantic similiarity\n",
    "* downsampling = downsample setting for frequent words. \n",
    "* seedie= can set a seed for reproducibility, note you also cannot use multiple workers if you want a fully reproducible model.\n",
    "* sg = learning archietecture: skip-gram (1) or CBOW (0)\n",
    "* hs= training algorithm to speed up computations: hierarchical softmax (1) or negative sampling (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Models with Four Combos of Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying out the four possible combinations on training data, I found that Model A performs the best on my training data and used this as my final\n",
    "model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences is an object of cleaned, pre-processed, tokenized text data\n",
    "sentences= open('your_tokenized_data_file_here.txt').read() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run bigram transformer, below, if you want bigrams (two-word phrases that are meaningful, like \"New York\"). If you don't want to bigrams, then just use \"sentences\" in training rather than bigram_transformer[sentences]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your \"sentences\" object with the cleaned text data. do this first before training model if you want to allow for bigrams in data (two word expressions, rather than only one word)\n",
    "bigram_transformer = phrases.Phrases(sentences) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try training models A, B, C, or D, which vary by learning architecture (Skip Gram vs Context Bag of Words) and training algorithm (negative sampling vs hierarchical softmax).\n",
    "Try other paramters, too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alina Arseniev\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "modelA_ALLYEARS= word2vec.Word2Vec(bigram_transformer[sentences], workers=4, sg=0,\n",
    "                          size=500, min_count=40,\n",
    "                          window=5, sample=1e-3)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "modelB_ALLYEARS= Word2Vec(bigram_transformer[sentences], workers=4, sg=1, \n",
    "                          size=500, min_count=40,\n",
    "                          window=10, sample=1e-3)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "modelC_ALLYEARS= Word2Vec(bigram_transformer[sentences],  workers=4, sg=0, hs=1,\n",
    "                          size=500, min_count=40, \n",
    "                          window=10, sample=1e-3)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "modelD_ALLYEARS= Word2Vec(bigram_transformer[sentences], workers=4, sg=1, hs=1,\n",
    "                          size=500, min_count=40,\n",
    "                          window=10, sample=1e-3)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelA_ALLYEARS.init_sims(replace=True) #Precompute L2-normalized vectors. If replace is set to TRUE, forget the original vectors and only keep the normalized ones = saves lots of memory, but can't continue to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelA_ALLYEARS.save(\"modelA_ALLYEARS_500dim_10CW\") #save your model for later use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Word2Vec Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload a trained model you want to evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "currentmodel=  Word2Vec.load(\"modelA_ALLYEARS_500dim_10CW\") #name of YOUR model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't have a model? Use a pretrained Word2Vec Model from Google, trained on Google News\n",
    "* Read and download here: https://code.google.com/archive/p/word2vec/\n",
    "* File is called \"GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make sure you have downloaded three files from this from the Github repo: questions_words_pasted.txt, testing.py, and question-words.txt\n",
    "accuracy=currentmodel.accuracy('questions_words_pasted.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "world_capitals1= accuracy[0] \n",
    "world_capitals2= accuracy[1] \n",
    "money= accuracy[2]\n",
    "US_capitals= accuracy[3]\n",
    "family= accuracy[4]\n",
    "adj_to_adverbs= accuracy[5]\n",
    "opposites= accuracy[6]\n",
    "comparative= accuracy[7]\n",
    "superlative= accuracy[8]\n",
    "present_particple= accuracy[9]\n",
    "nationality= accuracy[10]\n",
    "past_tense= accuracy[11]\n",
    "plural = accuracy[12]\n",
    "plural_verbs= accuracy[13]\n",
    "word_capitals3= accuracy[14] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5684210526315789\n",
      "0.4547244094488189\n",
      "0.1\n",
      "0.1790794979079498\n",
      "0.8973684210526316\n",
      "0.1431451612903226\n",
      "0.29210526315789476\n",
      "0.8536036036036037\n",
      "0.7210144927536232\n",
      "0.7149425287356321\n",
      "0.5718206770356816\n",
      "0.6518218623481782\n",
      "0.6857142857142857\n",
      "0.7321937321937322\n",
      "0.5711067051189618\n"
     ]
    }
   ],
   "source": [
    "for i in accuracy:\n",
    "    sum_corr = len(i['correct'])\n",
    "    sum_incorr = len(i['incorrect'])\n",
    "    total = sum_corr + sum_incorr\n",
    "    print(float(sum_corr)/(total))\n",
    "\n",
    "#print('Total sentences: {}, Correct: {:.2f}%, Incorrect: {:.2f}%'.format(total, percent(sum_corr), percent(sum_incorr)))\n",
    "\n",
    "#print sum_corr[1]\n",
    "#print sum_incorr[1]\n",
    "#print total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Word2Vec Model Exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload a trained model you want to explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "currentmodel=  Word2Vec.load(\"modelA_ALLYEARS_500dim_10CW\") #name of YOUR model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't have a model? Use a pretrained Word2Vec Model from Google, trained on Google News\n",
    "* Read and download here: https://code.google.com/archive/p/word2vec/\n",
    "* File is called \"GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currentmodel['woman'] #this gives the word-vector for 'woman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.754645441633\n"
     ]
    }
   ],
   "source": [
    "result = 1 - spatial.distance.cosine(currentmodel['computer'], currentmodel['software']) #calculate cosine similarity more manually, 2x checked against model.most_similar and results match\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('obese', 0.8001347780227661),\n",
       " ('underweight', 0.6493887901306152),\n",
       " ('normal_weight', 0.5515424609184265),\n",
       " ('being_overweight', 0.5171246528625488),\n",
       " ('anorexic', 0.5130723714828491)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentmodel.most_similar('overweight', topn=5) #asks which words are closest to overweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lab_tests', 0.24826177954673767),\n",
       " ('sterilization', 0.2312188297510147),\n",
       " ('endocrinologist', 0.2287030965089798),\n",
       " ('bouvia', 0.22845697402954102),\n",
       " ('auschwitz', 0.2241036295890808),\n",
       " ('legal_rights', 0.22197261452674866),\n",
       " ('antidepressant_drugs', 0.21886323392391205),\n",
       " ('pliny', 0.21702751517295837),\n",
       " ('medically', 0.21255454421043396),\n",
       " ('were_randomly', 0.206563338637352)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentmodel.most_similar(negative=['big']) #asks which words are FARTHEST from big, doesn't work so well. Shows how distance as can break down as a meaninful measure in this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hypertension', 0.5848986506462097), ('gout', 0.5804318785667419), ('neurological', 0.5797110795974731), ('congenital', 0.5704867839813232), ('diabetes', 0.5627163648605347), ('atherosclerosis', 0.5601633191108704), ('infectious', 0.5572311878204346), ('retardation', 0.5475963950157166), ('cognition', 0.5436995029449463), ('gastrointestinal', 0.5427045226097107)]\n",
      "break\n",
      "[('diabetes', 0.6686073541641235), ('pregnancy', 0.6431320905685425), ('estrogen', 0.6330111622810364), ('childbirth', 0.6287267804145813), ('asthma', 0.6252803802490234), ('alcoholism', 0.6236932277679443), ('anorexia', 0.619240403175354), ('reproductive', 0.6185950040817261), ('syndrome', 0.6179408431053162), ('diseases', 0.615617573261261)]\n"
     ]
    }
   ],
   "source": [
    "print currrentmodel.most_similar(positive=['man', 'obesity'], negative=['woman']) #man:king as woman:_?___\n",
    "print \" \"\n",
    "print currentmodel.most_similar(positive=['woman', 'obesity'], negative=['man']) #man:king as woman:_?___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noodle'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentmodel.doesnt_match(\"noodle chicken turkey beef\".split()) #A TRICK! My models seems to do pretty well when I try to trick them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Raw Word/Word-Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could do k-means on this (but computationally expensive), explore the models' bases vectors as initially learned, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordvecs= currentmodel.syn0 #if run, this outputs all word-vectors en masse http://rare-technologies.com/word2vec-tutorial/, but doesn't have name of word attached to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = open('test.csv', 'wb') #writes a csv with a row for each word-vector, total 17,855 rows. But, this data doesn't include word represented by each word-vector. \n",
    "a = csv.writer(b)\n",
    "a.writerows(wordvecs)\n",
    "b.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17855"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(currentmodel.vocab) #this is the length of the list of all words in the Word2Vec model's vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
