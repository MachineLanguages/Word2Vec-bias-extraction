{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notebook to accompany paper in preparation by Arseniev-Koehler and Foster.\n",
    "All code last checked on Python 3 in Windows 6/11/2018. Please do not cite or reuse this code yet. This code is still in preparation and may contain errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# More Manly or Womanly? Measure Biases in Word2Vec Models with a Support Vector Machine\n",
    "\n",
    "This project explores how language in the news is loaded with meanings of gender, morality, healthiness, and socio-economic status (SES). For example, which words are more masculine or feminine? Are certain words loaded with meanings of immorality or morality? \n",
    "\n",
    "In this code, we develop and then train a model to classify words with respect to each of these four dimensions (gender, morality, healthiness, and SES) on a set of training words. Then, we test model performance on a fresh set of testing words. \n",
    "\n",
    "Finally, we look at how language about body weight, such as \"obese\" and \"slender,\"  to see how these words are connoted with gender, morality, health, and social class. You might use this code to look at meanings of langauge in other arenas too - such as occupations, academic disciplines, or food. You might also extend this code to other types of meaning, or to other data sources. \n",
    "\n",
    "This notebook, Part B of our project, uses classical machine-learning methods, like a Support Vector Machine (SVM). For two alternate methods to check the robustness of your findings, look at code for [Part B and Part C](https://github.com/arsena-k/Word2Vec-bias-extraction). \n",
    "\n",
    "We start by loading up a trained Word2Vec model on news. We suggest a pre-trained model if you don't have one, or see [Part A](https://github.com/arsena-k/Word2Vec-bias-extraction) of this project for a tutorial on training your own Word2Vec model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "* Part 1. [Load up libraries and a Word2Vec Model](#Starting)\n",
    "* Part 2: [Explanation of Classification Method](#Motivation)\n",
    "* Part 3: [Load up Training/Testing Words](#LoadUp)\n",
    "* Part 4: [Robustness Checks](#Robustness)\n",
    "* Part 5: [Visualize how this Dimension Classifies words according to Gender, Morality, Health, and SES](#Results)\n",
    "\n",
    "\n",
    "*This is a long notebook. You can skip Part 3 (robustness checks) if you want to get right to the results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Staring'></a> \n",
    "# Part 1. Load up libraries and a Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn\n",
    "import csv\n",
    "import statistics\n",
    "from sklearn import datasets, decomposition, preprocessing\n",
    "import gensim\n",
    "np.set_printoptions(threshold=np.inf) #do this if you want to print full output\n",
    "import os\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from pylab import xlim\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "%matplotlib inline\n",
    "cwd= os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load up a pretrained Word2Vec Model**\n",
    "\n",
    "*Don't have a model? Use a pretrained Word2Vec Model from Google, trained on Google News*\n",
    "* Download a pre-trained model on GoogleNews, find link to download on this [site](https://code.google.com/archive/p/word2vec/) or direct link to [download here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing).\n",
    "* Extract the files, and make sure you have the one called \"GoogleNews-vectors-negative300.bin.gz\" in your working directory. Your working directory is the folder where this Jupyter notebook is currently saved. Currently, the code assumes that your downloads folder is your working directory. \n",
    "* Some of the vocabulary words used in this notebook may not exist, since the vocabulary words used in this notebook were selected based on a model trained on the New York Times, however the code will still run fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   An example for a PC computer if your model is in your downloads folder, and you're using the Google model \n",
    "#currentmodel=  KeyedVectors.load_word2vec_format('C:/Users/Alina Arseniev/Downloads/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "#   An example for a Mac if your model is in your downloads folder, and you're using the Google model \n",
    "#currentmodel=  KeyedVectors.load_word2vec_format(\"~/Downloads/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "#   Example based on my set-up of folders:\n",
    "currentmodel=  KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Have your own Word2Vec model? Load it up below.*\n",
    "* Some of the vocabulary words used in this notebook may not exist, since the vocabulary words used in this notebook were selected based on a model trained on the New York Times, however the code will still run fine. You might consider curating the words more to your vocabulary, especially if many words are missing in your vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    An example for a PC computer if your model is in your downloads folder, and you're using a model named \"modelA_ALLYEARS_500dim_10CW\" \n",
    "#currentmodel=  Word2Vec.load(\"C:/Users/Alina Arseniev/Downloads/modelA_ALLYEARS_500dim_10CW\")\n",
    "\n",
    "#   An example for a Mac if your model is in your downloads folder, and you're using a model named \"modelA_ALLYEARS_500dim_10CW\" \n",
    "#currentmodel=  Word2Vec.load(\"~/Downloads/modelA_ALLYEARS_500dim_10CW\")\n",
    "\n",
    "#   Example based on my set-up of folders:\n",
    "currentmodel=  Word2Vec.load(\"modelA_ALLYEARS_300dim_10CW\") #load up a trained Word2Vec model. You'll need to tailor this path to your computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Motivation'></a> \n",
    "# Part 2. Explanation of The Classification Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method trains a four machine-learning classifiers (mainly,  **Support Vector Machines**, or SVM) to classify words according to four dimensions: as feminine/masculine, moral/immoral, healthy/unhealthy, and high/low socio-econonic status. We feed in a training set of word-vectors we strongly expect to lie at one end of the dimensions or the other. For each of the four dimensions, a SVM model *learns* a hyperplane separate the two classes. \n",
    "\n",
    "A limitation of this machine-learning method, in contrast with the geomteric classification methods in Parts C and C, is that SVM models **risk being overparametertrized and overfitted.** This is because word-vectors tend to be a few hundred-dimensions and we have usually only have around 100 training words to learn a dimensions, so we often have more \"features\" than \"training examples.\" Cross-validation and the use of a fresh set of testing words helps us understand how much our model may be overfitted. Using cross-validation, we found that using PCA to reduce the dimensionality of word-vectors further did *not* improve results. We also only use a linear SVM model, and try out other possible machine-learning models (like a random forest) as comparison machine-learned models. \n",
    "\n",
    "* In our experiments, we did find that the SVM model to classify gender **initially did not corroborate our results** about how langauge around body weight is gendered, compared to the classification methods in Parts B and C. In fact, langauge about body weight was seemingly classified without pattern. This was also surprising to us given the large body of qualitaitve literature suggesting how our concepts of body weight is gendered. We speculated that this mismatch may be because gender training words are the most \"sharply\" or \"widely\" separated - **our training words were too easy**. Indeed, many of our training words for gender are explicilty and largely defined by their gendered meanings, such as \"he\", \"him\", and \"man\", \"man\" and \"machismo.\"  In contrast, training words for other dimensions, like \"yuppie\" for classifying SES, carry more layers of meaning than just social class. \"Yuppie,\" for example, also carries meaning about age and urbanization. \n",
    "* Thus, the gendered differences in our training set may be *too easy* to classify, leading the model to overfit to these training cases. To test this, we added some noisy words into our training words for gender, using words we thought were more implicitly gendered (e.g., “independent” as masculine, and “dependent” as feminine). We used two versions of updated training words, to vary the number of noisy word-vectors added. In both cases, words about body weight were **now classified in ways that corroborated findings from our other two methods.** Testing accuracy also increased from XX% to XX%. When we used these revised training sets for the other two classification methods, there were no changes in initial empirical findings, suggesting that our other two methods are more robust to overfitting. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LoadUp'></a> \n",
    "# Part 3. Create a Dataset of Training and Testing Word-Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a function to select the training words to find a dimensions\n",
    "def select_training_set(trainingset): #options are: gender, moral, health, ses\n",
    "    #gender is the main training set used to extract gender\n",
    "    #gender_2 has fewer precise gender words like \"he\" vs \"she\" than set 1,  and some more noise via words that are gendered but less clearcut than Set1. This set was used for experimenting acorss different methods. \n",
    "    #gender_3 even fewer precise gender words like \"he\" vs \"she\" than set 1,  and same added noise as training set 2. This set was used for experimenting acorss different methods.\n",
    "    if trainingset=='gender':\n",
    "        pos_word_list=['womanly', 'my_wife', 'my_mom', 'my_grandmother', 'woman', 'women', 'girl', 'girls', 'her', 'hers', 'herself', 'she', \n",
    "            'lady', 'gal', 'gals', 'madame', 'ladies', 'lady',\n",
    "          'mother', 'mothers', 'mom', 'moms', 'mommy', 'mama', 'ma', 'granddaughter', 'daughter', 'daughters', 'aunt', 'godmother', \n",
    "          'grandma', 'grandmothers', 'grandmother', 'sister', 'sisters', 'aunts', 'stepmother', 'granddaughters', 'niece',\n",
    "          'fiancee', 'ex_girlfriend', 'girlfriends', 'wife', 'wives', 'girlfriend', 'bride', 'brides', 'widow',\n",
    "           'twin_sister', 'younger_sister', 'teenage_girl', 'teenage_girls', 'eldest_daughter','estranged_wife', 'schoolgirl',\n",
    "          'businesswoman', 'congresswoman' , 'chairwoman', 'councilwoman', 'waitress', 'hostess', 'convent', 'heiress', \n",
    "           'saleswoman', 'queen', 'queens', 'princess', 'nun' , 'nuns', 'heroine', 'actress', 'actresses', 'uterus', 'vagina', 'ovarian_cancer',\n",
    "           'maternal', 'maternity', 'motherhood', 'sisterhood', 'girlhood', 'matriarch', 'sorority', \n",
    "         'older_sister', 'oldest_daughter', 'stepdaughter']\n",
    "        neg_word_list=['manly', 'my_husband', 'my_dad','my_grandfather', 'man', 'men', 'boy', 'boys', 'him', 'his', 'himself', 'he', 'guy', 'dude',\n",
    "            'dudes', 'sir', 'guys', 'gentleman','father', 'fathers', 'dad', 'dads', 'daddy', 'papa', 'pa', 'grandson' , 'son', 'sons', 'uncle', 'godfather', \n",
    "           'grandpa', 'grandfathers', 'grandfather', 'brother', 'brothers' , 'uncles', 'stepfather', 'grandsons', 'nephew',\n",
    "           'fiance', 'ex_boyfriend', 'boyfriends', 'husband', 'husbands', 'boyfriend', 'groom', 'grooms', 'widower',\n",
    "            'twin_brother', 'younger_brother', 'teenage_boy', 'teenage_boys', 'eldest_son', 'estranged_husband', 'schoolboy',\n",
    "            'businessman', 'congressman', 'chairman', 'councilman', 'waiter', 'host', 'monastery', 'heir', 'salesman', \n",
    "            'king', 'kings', 'prince', 'monk', 'monks', 'hero', 'actor', 'actors', 'prostate', 'penis', 'prostate_cancer', \n",
    "           'paternal', 'paternity', 'fatherhood', 'brotherhood', 'boyhood', 'patriarch', 'fraternity', \n",
    "           'older_brother', 'oldest_son', 'stepson']\n",
    "        pos_word_replacement='woman' #here's the generic replacement for feminine words\n",
    "        neg_word_replacement='man' #here's the generic replacement for masculine words\n",
    "    elif trainingset=='moral':\n",
    "        pos_word_list= ['good', 'benevolent', 'nice', 'caring', 'conscientious', 'polite', 'fair', 'virtue', 'respect', 'responsible', \n",
    "            'selfless', 'unselfish', 'sincere', 'truthful', 'wonderful', 'justice', 'innocent', 'innocence',\n",
    "           'complement', 'sympathetic', 'virtue', 'right', 'proud', 'pride','respectful', 'appropriate', 'pleasing', 'pleasant', \n",
    "            'pure', 'decent', 'pleasant', 'compassion' , 'compassionate', 'constructive','graceful', 'gentle', 'reliable',\n",
    "           'careful', 'help', 'decent' , 'moral', 'hero', 'heroic', 'heroism', 'honest', 'honesty',\n",
    "           'selfless', 'humility', 'humble', 'generous', 'generosity', 'faithful', 'fidelity', 'worthy', 'tolerant',\n",
    "            'obedient', 'pious', 'saintly', 'angelic', 'virginal', 'sacred', 'reverent', 'god', 'hero', 'heroic', \n",
    "            'forgiving', 'saintly','holy', 'chastity', 'grateful', 'considerate', 'humane', \n",
    "            'trustworthy', 'loyal', 'loyalty', 'empathetic', 'empathy', 'clean', 'straightforward', 'pure']\n",
    "        neg_word_list= ['bad', 'evil', 'mean', 'uncaring', 'lazy', 'rude', 'unfair', 'sin', 'disrespect','irresponsible', \n",
    "           'self_centered', 'selfish', 'insincere', 'lying', 'horrible', 'injustice', 'guilty', 'guilt', \n",
    "            'insult', 'unsympathetic', 'vice', 'wrong', 'ashamed', 'shame', 'disrespectful', 'inappropriate', 'vulgar', 'crude', \n",
    "            'dirty', 'obscene', 'offensive', 'cruelty','brutal', 'destructive', 'rude', 'harsh', 'unreliable',\n",
    "            'careless', 'harm', 'indecent', 'immoral', 'coward', 'cowardly', 'cowardice', 'dishonest', 'dishonesty',\n",
    "            'narcissistic', 'arrogance', 'arrogant', 'greedy', 'greed', 'betray', 'betrayal', 'unworthy', 'intolerant', \n",
    "             'defiant', 'rebellious', 'demonic','devilish', 'promiscuous', 'profane', 'irreverent', 'devil', 'villain', 'villainous', \n",
    "            'vindictive', 'diabolical', 'unholy', 'promiscuity', 'ungrateful', 'thoughtless', 'inhumane',\n",
    "            'untrustworthy', 'treacherous', 'treachery', 'callous', 'indifference', 'dirty', 'manipulative', 'impure' ]\n",
    "        pos_word_replacement='moral' #here's the generic replacement for moral words\n",
    "        neg_word_replacement='immoral' #here's the generic replacement for immoral words\n",
    "    elif trainingset=='health':\n",
    "        pos_word_list= ['fertile', 'help_prevent', 'considered_safe', 'safer', 'healthy', 'healthy', 'healthy', 'healthy', 'healthy',\n",
    "            'healthful', 'well_balanced', 'natural', 'healthy', 'athletic','physically_active', 'health',\n",
    "            'health', 'nutritious','nourishing', 'stronger', 'strong','wellness', 'safe', 'nutritious_food','exercise',\n",
    "            'physically_fit', 'unprocessed', 'healthier_foods', 'nutritious_foods', 'nutritious', 'nutritious',\n",
    "           'healthy_eating', 'healthy_diet', 'healthy_diet', 'nourishing', 'nourished', 'regular_exercise', 'safety', 'safe', \n",
    "            'helpful', 'beneficial', 'healthy', 'healthy', 'sturdy', 'lower_risk', 'reduced_risk', 'decreased_risk', 'nutritious_foods', 'whole_grains', 'healthier_foods',\n",
    "            'healthier_foods', 'physically_active', 'physical_activity', 'nourished', 'vitality', 'energetic', 'able_bodied',\n",
    "            'resilience', 'strength', 'less_prone', 'sanitary', 'clean',  'healing', 'heal', 'salubrious']   \n",
    "        neg_word_list= ['infertile', 'cause_harm','potentially_harmful','riskier', 'unhealthy', 'sick', 'ill', 'frail', 'sickly', \n",
    "            'unhealthful','unbalanced', 'unnatural', 'dangerous', 'sedentary', 'inactive', 'illness', \n",
    "            'sickness', 'toxic', 'unhealthy', 'weaker', 'weak', 'illness', 'unsafe', 'unhealthy_foods', 'sedentary',\n",
    "            'inactive', 'highly_processed', 'processed_foods', 'junk_foods', 'unhealthy_foods', 'junk_foods',\n",
    "               'processed_foods', 'processed_foods', 'fast_food', 'unhealthy_foods', 'deficient', 'sedentary', 'hazard','hazardous', \n",
    "            'harmful', 'injurious',  'chronically_ill', 'seriously_ill', 'frail', 'higher_risk', 'greater_risk', 'increased_risk', 'fried_foods', 'fried_foods',\n",
    "            'fatty_foods', 'sugary_foods', 'sedentary', 'physical_inactivity', 'malnourished', 'lethargy', 'lethargic', 'disabled',\n",
    "            'susceptibility', 'weakness', 'more_susceptible', 'filthy', 'dirty', 'harming', 'hurt', 'deleterious']\n",
    "        pos_word_replacement='healthy' #here's the generic replacement for healthy words\n",
    "        neg_word_replacement='ill' #here's the generic replacement for unhealthy words\n",
    "    elif trainingset=='ses':\n",
    "        pos_word_list=['wealth', 'wealthier', 'wealthiest', 'affluence', 'prosperity', 'wealthy', 'affluent', 'affluent', 'prosperous',\n",
    "                'prosperous','prosperous','disposable_income',  'wealthy','suburban','luxurious','upscale','upscale', 'luxury', \n",
    "                'richest', 'privileged', 'moneyed', 'privileged', 'privileged', 'educated', 'employed', \n",
    "                'elite', 'upper_income', 'upper_class', 'employment', 'riches', 'millionaire', 'aristocrat', 'college_educated',\n",
    "                'abundant', 'lack', 'luxury', 'profitable', 'profit', 'well_educated', 'elites', 'heir', 'well_heeled', \n",
    "                'white_collar', 'higher_incomes', 'bourgeois', 'fortunate', 'successful','economic_growth', 'prosper', 'suburbanites']\n",
    "        neg_word_list= ['poverty', 'poorer', 'poorest', 'poverty', 'poverty', 'impoverished', 'impoverished',  'needy',  'impoverished',\n",
    "                 'poor', 'needy', 'broke', 'needy', 'slum', 'ghetto', 'slums', 'ghettos', 'poor_neighborhoods', \n",
    "                'poorest', 'underserved', 'disadvantaged','marginalized', 'underprivileged', 'uneducated', 'unemployed', \n",
    "                'marginalized', 'low_income', 'underclass','unemployment', 'rags', 'homeless', 'peasant', 'college_dropout', \n",
    "                'lacking', 'abundance', 'squalor', 'bankrupt', 'debt', 'illiterate' ,'underclass', 'orphan',  'destitute', \n",
    "                'blue_collar', 'low_income', 'neediest', 'less_fortunate', 'unsuccessful', 'economic_crisis', 'low_wage', 'homeless']\n",
    "        pos_word_replacement='wealthy' #here's the generic replacement for rich words\n",
    "        neg_word_replacement='poor' #here's the generic replacement for poor words\n",
    "    elif trainingset=='gender_2':\n",
    "        pos_word_list=[ 'girl', 'girls', 'her', 'hers', 'herself', 'she', \n",
    "            'lady', 'gal', 'gals', 'madame', 'ladies', 'lady',\n",
    "          'mother', 'mothers', 'mom', 'moms', 'mommy', 'mama', 'ma', 'granddaughter', 'daughter', 'daughters', 'aunt', 'godmother', \n",
    "          'grandma', 'grandmothers', 'grandmother', 'sister', 'sisters', 'aunts', 'stepmother', 'granddaughters', 'niece',\n",
    "        'fiancee', 'ex_girlfriend', 'girlfriends', 'wife', 'wives', 'girlfriend', 'bride', 'brides', 'widow',\n",
    "           'twin_sister', 'younger_sister', 'teenage_girl', 'teenage_girls', 'eldest_daughter','estranged_wife', 'schoolgirl',\n",
    "        'businesswoman', 'congresswoman' , 'chairwoman', 'councilwoman', 'waitress', 'hostess', 'convent', 'heiress', \n",
    "           'saleswoman', 'queen', 'queens', 'princess', 'nun' , 'nuns', 'heroine', 'actress', 'actresses', 'uterus', 'vagina', 'ovarian_cancer',\n",
    "        'maternal', 'maternity', 'motherhood', 'sisterhood', 'girlhood', 'matriarch', 'sorority', 'mare', 'hen', 'hens', 'filly', 'fillies',\n",
    "          'deer', 'older_sister', 'oldest_daughter', 'stepdaughter', 'pink',  'cute', 'dependent', 'nurturing', 'hysterical', 'bitch',  'dance', 'dancing'] \n",
    "        neg_word_list=['boy', 'boys', 'him', 'his', 'himself', 'he', 'guy', 'dude',\n",
    "            'dudes', 'sir', 'guys', 'gentleman','father', 'fathers', 'dad', 'dads', 'daddy', 'papa', 'pa', 'grandson' , 'son', 'sons', 'uncle', 'godfather', \n",
    "        'grandpa', 'grandfathers', 'grandfather', 'brother', 'brothers' , 'uncles', 'stepfather', 'grandsons', 'nephew',\n",
    "           'fiance', 'ex_boyfriend', 'boyfriends', 'husband', 'husbands', 'boyfriend', 'groom', 'grooms', 'widower',\n",
    "            'twin_brother', 'younger_brother', 'teenage_boy', 'teenage_boys', 'eldest_son', 'estranged_husband', 'schoolboy',\n",
    "            'businessman', 'congressman', 'chairman', 'councilman', 'waiter', 'host', 'monastery', 'heir', 'salesman', \n",
    "            'king', 'kings', 'prince', 'monk', 'monks', 'hero', 'actor', 'actors', 'prostate', 'penis', 'prostate_cancer', \n",
    "        'paternal', 'paternity', 'fatherhood', 'brotherhood', 'boyhood', 'patriarch', 'fraternity', 'stallion', 'rooster', 'roosters', 'colt',\n",
    "           'colts', 'bull', 'older_brother', 'oldest_son', 'stepson', 'blue' ,'manly', 'independent', 'aggressive', 'angry', 'jerk', 'wrestle', 'wrestling'  ]\n",
    "        pos_word_replacement='woman'\n",
    "        neg_word_replacement='man'\n",
    "    elif trainingset=='gender_3':\n",
    "        pos_word_list=['madame', 'ladies', 'lady',\n",
    "          'mother', 'mothers', 'mom', 'mama', 'granddaughter', 'daughter', 'daughters', 'aunt', 'godmother', \n",
    "          'grandma', 'grandmothers', 'grandmother', 'sister', 'sisters', 'aunts', 'stepmother', 'granddaughters', 'niece',\n",
    "        'fiancee', 'ex_girlfriend', 'girlfriends', 'wife', 'wives', 'girlfriend', 'bride', 'brides', 'widow',\n",
    "           'twin_sister', 'younger_sister', 'teenage_girl', 'teenage_girls', 'eldest_daughter','estranged_wife', 'schoolgirl',\n",
    "        'businesswoman', 'congresswoman' , 'chairwoman', 'councilwoman', 'waitress', 'hostess', 'convent', 'heiress', \n",
    "           'saleswoman', 'queen', 'queens', 'princess', 'nun' , 'nuns', 'heroine', 'actress', 'actresses', 'uterus', 'vagina', 'ovarian_cancer',\n",
    "        'maternal', 'maternity', 'motherhood', 'sisterhood', 'girlhood', 'matriarch', 'sorority', 'mare', 'hen', 'hens', 'filly', 'fillies',\n",
    "          'deer', 'older_sister', 'oldest_daughter', 'stepdaughter', 'pink', 'cute', 'dependent', 'nurturing', 'hysterical', 'bitch',  'dance', 'dancing']\n",
    "        neg_word_list=['sir', 'guys', 'gentleman','father', 'fathers', 'dad', 'papa', 'grandson' , 'son', 'sons', 'uncle', 'godfather', \n",
    "        'grandpa', 'grandfathers', 'grandfather', 'brother', 'brothers' , 'uncles', 'stepfather', 'grandsons', 'nephew',\n",
    "           'fiance', 'ex_boyfriend', 'boyfriends', 'husband', 'husbands', 'boyfriend', 'groom', 'grooms', 'widower',\n",
    "            'twin_brother', 'younger_brother', 'teenage_boy', 'teenage_boys', 'eldest_son', 'estranged_husband', 'schoolboy',\n",
    "            'businessman', 'congressman', 'chairman', 'councilman', 'waiter', 'host', 'monastery', 'heir', 'salesman', \n",
    "            'king', 'kings', 'prince', 'monk', 'monks', 'hero', 'actor', 'actors', 'prostate', 'penis', 'prostate_cancer', \n",
    "        'paternal', 'paternity', 'fatherhood', 'brotherhood', 'boyhood', 'patriarch', 'fraternity', 'stallion', 'rooster', 'roosters', 'colt',\n",
    "           'colts', 'bull', 'older_brother', 'oldest_son', 'stepson', 'blue' ,'manly', 'independent', 'aggressive', 'angry', 'jerk', 'wrestle', 'wrestling'  ]\n",
    "        pos_word_replacement='woman'\n",
    "        neg_word_replacement='man'\n",
    "    elif trainingset=='gender_4':\n",
    "        pos_word_list=['madame', 'ladies', 'lady',\n",
    "          'mother', 'mothers', 'mom', 'mama', 'granddaughter', 'daughter', 'daughters', 'aunt', 'godmother', \n",
    "          'grandma', 'grandmothers', 'grandmother', 'sister', 'sisters', 'aunts', 'stepmother', 'granddaughters', 'niece',\n",
    "        'fiancee', 'ex_girlfriend', 'girlfriends', 'wife', 'wives', 'girlfriend', 'bride', 'brides', 'widow',\n",
    "           'twin_sister', 'younger_sister', 'teenage_girl', 'teenage_girls', 'eldest_daughter','estranged_wife', 'schoolgirl',\n",
    "        'businesswoman', 'congresswoman' , 'chairwoman', 'councilwoman', 'waitress', 'hostess', 'convent', 'heiress', \n",
    "           'saleswoman', 'queen', 'queens', 'princess', 'nun' , 'nuns', 'heroine', 'actress', 'actresses', 'uterus', 'vagina', 'ovarian_cancer',\n",
    "        'maternal', 'maternity', 'motherhood', 'sisterhood', 'girlhood', 'matriarch', 'sorority', 'mare', 'hen', 'hens', 'filly', 'fillies',\n",
    "          'deer', 'older_sister', 'oldest_daughter', 'stepdaughter', 'pink', 'cute', 'dependent', 'nurturing', 'hysterical', 'bitch',  'dance', 'dancing']\n",
    "        neg_word_list=['sir', 'guys', 'gentleman','father', 'fathers', 'dad', 'papa', 'grandson' , 'son', 'sons', 'uncle', 'godfather', \n",
    "        'grandpa', 'grandfathers', 'grandfather', 'brother', 'brothers' , 'uncles', 'stepfather', 'grandsons', 'nephew',\n",
    "           'fiance', 'ex_boyfriend', 'boyfriends', 'husband', 'husbands', 'boyfriend', 'groom', 'grooms', 'widower',\n",
    "            'twin_brother', 'younger_brother', 'teenage_boy', 'teenage_boys', 'eldest_son', 'estranged_husband', 'schoolboy',\n",
    "            'businessman', 'congressman', 'chairman', 'councilman', 'waiter', 'host', 'monastery', 'heir', 'salesman', \n",
    "            'king', 'kings', 'prince', 'monk', 'monks', 'hero', 'actor', 'actors', 'prostate', 'penis', 'prostate_cancer', \n",
    "        'paternal', 'paternity', 'fatherhood', 'brotherhood', 'boyhood', 'patriarch', 'fraternity', 'stallion', 'rooster', 'roosters', 'colt',\n",
    "           'colts', 'bull', 'older_brother', 'oldest_son', 'stepson', 'blue' ,'manly', 'independent', 'aggressive', 'angry', 'jerk', 'wrestle', 'wrestling'  ]\n",
    "        pos_word_replacement='woman'\n",
    "        neg_word_replacement='man'\n",
    "    \n",
    "    pos_words=[]\n",
    "    neg_words=[]\n",
    "    pos_word_list_checked=[]\n",
    "    neg_word_list_checked=[]\n",
    "    for i in pos_word_list:\n",
    "        try:\n",
    "            pos_words.append(currentmodel[i])\n",
    "            pos_word_list_checked.append(i)\n",
    "        except KeyError:\n",
    "            #print(str(i) +  ' was not in this Word2Vec models vocab, and has been replaced with: ' + str(pos_word_replacement) ) #uncomment this to be alerted each time a pos training word-vector is replaced\n",
    "            pos_words.append(currentmodel[pos_word_replacement])\n",
    "            pos_word_list_checked.append(pos_word_replacement)\n",
    "    for i in neg_word_list:\n",
    "        try:\n",
    "            neg_words.append(currentmodel[i])\n",
    "            neg_word_list_checked.append(i)\n",
    "        except KeyError:\n",
    "            #print(str(i) +  ' was not in this Word2Vec models vocab, and has been replaced with: ' + str(neg_word_replacement) ) #uncomment this to be alerted each time a neg training word-vector is replaced\n",
    "            neg_words.append(currentmodel[neg_word_replacement])\n",
    "            neg_word_list_checked.append(neg_word_replacement)\n",
    "\n",
    "    print('\\033[1m' + \"Number of pos train words: \"+ '\\033[0m' + str(len(pos_words)) + '\\033[1m' + \" Number of neg train words: \" + '\\033[0m' + str(len(neg_words)) )\n",
    "    train_classes= np.concatenate((np.array(np.repeat(1, len(pos_words))), np.array(np.repeat(0, len(neg_words))))) #1 is feminine/moral/healthy/rich by default 0 is masculine/immoral/unhealthy/poor by default    \n",
    "    words= np.concatenate((np.asarray(pos_words), np.asarray(neg_words)))\n",
    "    words= preprocessing.normalize(np.asarray(words), norm='l2')\n",
    "    pos_word_list_checked.extend(neg_word_list_checked) #pos_word_list now includes neg words\n",
    "    \n",
    "    return(pos_word_list_checked, words, train_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_testing_set(testingset):\n",
    "    if testingset=='gender':\n",
    "        test_word_list= ['goddess', 'single_mother', 'girlish', 'feminine', 'young_woman', 'little_girl', 'ladylike', 'my_mother', \n",
    "           'teenage_daughter', 'mistress', 'great_grandmother', 'adopted_daughter', 'femininity', 'motherly', 'matronly', \n",
    "           'showgirl', 'housewife', 'vice_chairwoman', 'co_chairwoman', 'spokeswoman', 'governess', 'divorcee', 'spinster', \n",
    "           'maid', 'countess', 'pregnant_woman', 'landlady', 'seamstress', 'young_girl', 'waif', 'femme_fatale','comedienne',\n",
    "            'boyish', 'masculine',  'lad', 'policeman', 'macho', 'gentlemanly', 'machismo',  'teenage_son', \n",
    "            'beau', 'great_grandfather', 'tough_guy', 'masculinity', 'bad_boy', 'spokesman', 'baron', 'adult_male', 'landlord', 'fireman', 'mailman', 'vice_chairman', \n",
    "           'co_chairman','young_man', 'bearded', 'mustachioed', 'con_man', 'homeless_man', 'gent', 'strongman']\n",
    "        test_classes=np.repeat(1, 32).tolist() #1 is feminine\n",
    "        masc2=np.repeat(0, 28).tolist() #0 is masculine\n",
    "        for i in masc2:\n",
    "            test_classes.append(i) \n",
    "    elif testingset=='moral':\n",
    "        test_word_list= ['great', 'best', 'faith', 'chaste', 'wholesome', 'noble', 'honorable', 'immaculate', 'gracious', \n",
    "           'courteous', 'delightful', 'earnest', 'amiable', 'admirable', 'disciplined', 'patience', 'integrity',\n",
    "            'restraint', 'upstanding', 'diligent', 'dutiful', 'loving', 'righteous','respectable', 'praise', 'devout', 'forthright',\n",
    "            'depraved', 'repulsive', 'repugnant', 'corruption', 'vicious', 'unlawful', 'outrage',  'shameless', 'perverted',\n",
    "            'filthy', 'lewd', 'subversive', 'sinister', 'murderous', 'perverse', \n",
    "           'monstrous', 'homicidal', 'indignant', 'misdemeanor', 'degenerate', 'malevolent', 'illegal','terrorist','terrorism',  \n",
    "             'cheated', 'vengeful', 'culpable','vile', 'hateful', 'abuse', 'abusive', 'criminal', 'deviant']\n",
    "        test_classes=np.repeat(1, 27 ).tolist() #1 is feminine\n",
    "        masc2=np.repeat(0,33).tolist() #0 is masculine\n",
    "        for i in masc2:\n",
    "            test_classes.append(i)\n",
    "    elif testingset=='health':\n",
    "        test_word_list= [ 'balanced_diet', 'healthfulness', 'fiber', 'jogging', 'stopping_smoking', 'vigor', \n",
    "          'active', 'fit', 'flourishing', 'sustaining', 'hygienic', 'hearty', 'enduring', 'energized', 'wholesome', \n",
    "           'holistic', 'healed', 'fitter', 'health_conscious', 'more_nutritious', 'live_longer',  'exercising_regularly',\n",
    "           'healthier_choices', 'healthy_habits', 'healthy_lifestyle', 'healthful_eating', 'immune', \n",
    "            'deadly', 'diseased',  'adverse', 'risky', 'fatal', 'filthy', 'epidemic', 'crippling', 'carcinogenic', 'carcinogen',\n",
    "           'crippled', 'afflicted', 'contaminated', 'fatigued', 'detrimental', 'bedridden', 'incurable', 'hospitalized',\n",
    "           'infected', 'ailing', 'debilitated', 'poisons', 'disabling', 'life_threatening', 'debilitating', \n",
    "           'chronic_illness', 'artery_clogging', 'hypertension','disease', 'stroke',\n",
    "            'plague', 'fatty', 'smoking']\n",
    "        test_classes=np.repeat(1, 27).tolist() #1 is feminine\n",
    "        masc2=np.repeat(0, 33 ).tolist() #0 is masculine\n",
    "        for i in masc2:\n",
    "            test_classes.append(i) \n",
    "    elif testingset=='ses':\n",
    "        test_word_list= ['rich', 'billionaire', 'banker',  'fortune', 'heiress', 'cosmopolitan', 'ornate', 'entrepreneur', 'sophisticated',\n",
    "                'aristocratic', 'investor', 'highly_educated', 'better_educated',  'splendor', \n",
    "               'businessman', 'opulent', 'multimillionaire', 'philanthropist', 'estate', 'estates', 'chateau', 'fortunes', \n",
    "               'financier', 'young_professionals','tycoon', 'baron', 'grandeur', 'magnate', \n",
    "               'investment_banker', 'venture_capitalist', 'upwardly_mobile', 'highly_skilled', 'yuppies', 'genteel',\n",
    "                         'homelessness', 'ruin', 'ruined', 'downtrodden', 'less_affluent',\n",
    "                'housing_project', 'homeless_shelters', 'indigent', 'jobless', 'welfare',  \n",
    "                'temporary_shelters','housing_projects', 'subsidized_housing', 'starving', 'beggars', 'orphanages',\n",
    "                'dispossessed', 'uninsured', 'welfare_recipients', 'food_stamps', \n",
    "                'malnutrition',  'underemployed', 'disenfranchised', 'servants', 'displaced', 'poor_families'] \n",
    "        test_classes=np.repeat(1, 34).tolist()#1 is feminine\n",
    "        masc2=np.repeat(0, 26).tolist() #0 is masculine\n",
    "        for i in masc2:\n",
    "            test_classes.append(i) \n",
    "    elif testingset=='gender_stereotypes':\n",
    "        test_word_list=['petite', 'cooking', 'graceful',  'housework', 'soft', 'whisper', 'flirtatious', 'accepting', 'blonde', 'blond', 'doll', 'dolls','nurse',  'estrogen', 'lipstick','pregnant', 'nanny', 'pink', \n",
    "                 'sewing', 'modeling', 'dainty', 'gentle', 'children','pregnancy', 'nurturing', 'depressed', 'nice', 'emotional','depression', 'home', 'kitchen', 'quiet', 'submissive',\n",
    "                   'soldier', 'army', 'drafted', 'military',   'beard', 'mustache', 'genius', 'engineering', 'math', \n",
    "                  'brilliant', 'strong', 'strength',  'politician', 'programmer','doctor', 'sexual', 'aggressive', \n",
    "                    'testosterone', 'tall', 'competitive', 'big', 'powerful', 'mean', 'sports', 'fighting', 'confident', 'rough', 'loud', 'worldly',\n",
    "                   'experienced', 'insensitive', 'ambitious', 'dominant']\n",
    "        test_classes=np.repeat(1, 33 ).tolist() #1 is feminine\n",
    "        masc2=np.repeat(0,33).tolist() #0 is masculine\n",
    "        for i in masc2:\n",
    "            test_classes.append(i) \n",
    "    else:\n",
    "        print('choose a testing set: gender, moral, health, or ses')\n",
    "        \n",
    "    test_words=[]\n",
    "    test_word_list_checked=[]\n",
    "    test_classes_checked=[] \n",
    "    for i in test_word_list:\n",
    "        try:\n",
    "            test_words.append(currentmodel[i])\n",
    "            test_word_list_checked.append(i)\n",
    "            test_classes_checked.append(test_classes[test_word_list.index(i)]) \n",
    "        except KeyError:\n",
    "            continue\n",
    "            #print(str(i) +  ' was not in this Word2Vec models vocab, and has been removed as a test word') #uncomment this to be alerted each time a test word is not included in your model's vocabulary\n",
    "            #index_missing= test_word_list.index(i) #new\n",
    "            #del(test_classes[index_missing]) \n",
    "            #test_words.append(currentmodel[test_word_replacement])\n",
    "            #test_word_list_checked.append(test_word_replacement)\n",
    "            #get index of word, and remove this from classes, and do not append to list of vectors and word-list\n",
    "\n",
    "    test_words= preprocessing.normalize(np.asarray(test_words), norm='l2')\n",
    "\n",
    "    test_classes_checked=np.asarray(test_classes_checked)\n",
    "    print('\\033[1m'+ \"Number of test words in model vocabulary, out of 60: \" + '\\033[0m' + str(len(test_words)))\n",
    "    return(test_word_list_checked, test_words, test_classes_checked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Robustness'></a> \n",
    "# Part 4. Robustness Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the dimension you are interested in (gender, moral, health, or ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNumber of pos train words: \u001b[0m85\u001b[1m Number of neg train words: \u001b[0m85\n"
     ]
    }
   ],
   "source": [
    "train_word_list_checked, train_words, train_classes= select_training_set('gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do **cross validation** to see how accuracy changes at classifying words if we just use a subset of training words to extract the dimension and and look at how the dimension classifies that subset of words and how it classifes the held-out words. This tells us, for example, how robust our methods are to our word choices. It is also a way we can try different models, and different parametrizations of models, and see how overfitted our models are.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMean Accuracy across Training Subsets:\u001b[0m0.952832438879\n",
      "\u001b[1mStandard Deviation of Accuracy across Training Subsets:\u001b[0m0.0033484709842960454\n",
      "\u001b[1mMean Accuracy across Held-Out Subsets: \u001b[0m0.884615384615\n",
      "\u001b[1mStandard Deviation of Accuracy across Held-Out Subsets: \u001b[0m0.320721458628893\n"
     ]
    }
   ],
   "source": [
    "#cross validation\n",
    "kf= KFold(n_splits=len(words), shuffle=True)  \n",
    "#n_splits written here is leave-one-word-out cross validation, which is the maximum for n_splits. Try various quantities of n_splits. \n",
    "\n",
    "#NOTE that we are only using our training words here, but dividing it into a \"sub\" training set, and an unseen set of the trainign words, we call in this code block the \"test\" set. But we do not use our unseen true \"test\" words until later on. \n",
    "\n",
    "trainacc=[] \n",
    "testacc=[] \n",
    "\n",
    "for train_index, test_index in kf.split(words): #only need the indices on pos words or neg words, then will be the same indices to use for both    \n",
    "    \n",
    "    clf=svm.SVC(kernel='linear', C=1) #Use linear kernel, since not much data. More complex kernels performed worse and very high SD on accuracy. #C is 1 by default and it’s a reasonable default choice. If you have a lot of noisy observations you should decrease it. It corresponds to regularize more the estimation.\n",
    "    #clf = RandomForestClassifier(n_estimators=100, max_depth=3, max_features=None, random_state=234) #max_features=None means all features are tried, rather than a sample. So the only randomness is the data. Default is that a sample of sqrt(n_features) is tried out for each tree, but this doesn't perform as well on training data, and doesn't make sense theoretically since I expect there is a few specific features that carry most of gender information. I tried betwen max depth of 2-4; 3 seems best for gender but on smaller sets consider 2.\n",
    "    #clf = MLPClassifier(hidden_layer_sizes=(5)) #to try neural network rather than SVM, but really not enough data here, its just a sample of how to change the ML classifier here. \n",
    "    clf= clf.fit(train_words[train_index], train_classes[train_index] )\n",
    "    \n",
    "    #Now get predictions on the \"training\" set of the fold\n",
    "    predictions_training=clf.predict(train_words[train_index])\n",
    "    trainacc.append(accuracy_score(train_classes[train_index], predictions_training)) #append accuracy from this specific training subset\n",
    "\n",
    "    #Now get predictions on subset of unseen trainning words (i.e., validation set)\n",
    "    predictions_testing=clf.predict(train_words[test_index])\n",
    "    testacc.append(accuracy_score(train_classes[test_index], predictions_testing)) #append accuracy from this specific 'testing' subset\n",
    "\n",
    "    \n",
    "print('\\033[1m' +'Mean Accuracy across Training Subsets:'  + '\\033[0m'+ str(statistics.mean(trainacc)))\n",
    "print('\\033[1m' +'Standard Deviation of Accuracy across Training Subsets:'  + '\\033[0m'+ str(statistics.stdev(trainacc)))\n",
    "print('\\033[1m' +  'Mean Accuracy across Held-Out Subsets: ' + '\\033[0m'+ str(statistics.mean(testacc)))\n",
    "print('\\033[1m' +'Standard Deviation of Accuracy across Held-Out Subsets: ' + '\\033[0m' + str(statistics.stdev(testacc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out different types of machine-learning classifers on all training data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try SVM on all training data, still using dimension selected above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAccuracy on Training Data with SVM Classifier:\u001b[0m0.941176470588\n"
     ]
    }
   ],
   "source": [
    "clf=svm.SVC(kernel='linear', C=1) #after grid search, for Gender it seems that between C=1 to C=5 is ideal, and C=3 is best\n",
    "clf = clf.fit(train_words, train_classes)\n",
    "predictions =clf.predict(train_words) #consider using proba rather than binary\n",
    "print('\\033[1m' +'Accuracy on Training Data with SVM Classifier:'  + '\\033[0m'+ str(accuracy_score(train_classes, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Decision Tree on all training data, still using dimension selected above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAccuracy on Training Data with Decision Tree Classifier:\u001b[0m0.917647058824\n"
     ]
    }
   ],
   "source": [
    "#fit tree with chosen depth, for training set\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3) #tried betwen max depth of 2-4; 3 seems best for gender but on smaller sets consider 2.\n",
    "clf = clf.fit(train_words, train_classes)\n",
    "predictions =clf.predict(train_words) #consider using proba rather than binary\n",
    "print('\\033[1m' +'Accuracy on Training Data with Decision Tree Classifier:'  + '\\033[0m'+ str(accuracy_score(train_classes, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a Random Forest on all training data, still using dimension selected above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAccuracy on Training Data with a Random Forest Classifier:\u001b[0m0.994117647059\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=3, max_features=None, random_state=234) #max_features=None means all features are tried, rather than a sample. So the only randomness is the data. Default is that a sample of sqrt(n_features) is tried out for each tree, but this doesn't perform as well on training data, and doesn't make sense theoretically since I expect there is a few specific features that carry most of gender information. I tried betwen max depth of 2-4; 3 seems best for gender but on smaller sets consider 2.\n",
    "clf = clf.fit(train_words, train_classes)\n",
    "predictions =clf.predict(train_words) #consider using proba rather than binary\n",
    "print('\\033[1m' +'Accuracy on Training Data with a Random Forest Classifier:'  + '\\033[0m'+ str(accuracy_score(train_classes, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Results'></a> \n",
    "# Part 5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the dimension you're interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNumber of pos train words: \u001b[0m80\u001b[1m Number of neg train words: \u001b[0m80\n",
      "\u001b[1mNumber of test words in model vocabulary, out of 60: \u001b[0m60\n"
     ]
    }
   ],
   "source": [
    "train_word_list_checked, train_words, train_classes= select_training_set('gender_3')  #note that many of these dimensions are likely picking up similar signal of \"valence.\" We can see this, in part, because if we use a trainig set from one dimensiion we still do well on the testing set form another dimensions.  In Parts B and C we saw that the cosine smilarity of the extracted dimension was not 1, meaning that these methods are NOT picking up the same thing. Its a little less clear with this ML method, but this ML method is still an interesting experiment.  \n",
    "test_word_list_checked, test_words, test_classes = select_testing_set('gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m% Accuracy on Training Data with SVM Classifier:\u001b[0m0.98125\n",
      "\u001b[1m% Accuracy on Testing Data with SVM Classifier:\u001b[0m0.983333333333\n",
      "\u001b[1mN Accuracy on Training Data with SVM Classifier:\u001b[0m157\n",
      "\u001b[1mN Accuracy on Testing Data with SVM Classifier:\u001b[0m59\n"
     ]
    }
   ],
   "source": [
    "clf=svm.SVC(kernel='linear', C=1, probability=True) #after grid search, for Gender it seems that between C=1 to C=5 is ideal, and C=3 is best\n",
    "clf = clf.fit(train_words, train_classes)\n",
    "train_predictions =clf.predict(train_words) #consider using proba rather than binary\n",
    "train_proba_predictions =clf.predict_proba(train_words) \n",
    "\n",
    "#clf = clf.fit(test_words, test_classes)\n",
    "test_predictions =clf.predict(test_words) #consider using proba rather than binary\n",
    "test_proba_predictions =clf.predict_proba(test_words) \n",
    "\n",
    "print('\\033[1m' +'% Accuracy on Training Data with SVM Classifier:'  + '\\033[0m'+ str(accuracy_score(train_classes, train_predictions)) )\n",
    "print('\\033[1m' +'% Accuracy on Testing Data with SVM Classifier:'  + '\\033[0m'+ str(accuracy_score(test_classes, test_predictions)) )\n",
    "\n",
    "print('\\033[1m' +'N Accuracy on Training Data with SVM Classifier:'  + '\\033[0m'+ str(accuracy_score(train_classes, train_predictions, normalize=False)) )\n",
    "print('\\033[1m' +'N Accuracy on Testing Data with SVM Classifier:'  + '\\033[0m'+ str(accuracy_score(test_classes, test_predictions, normalize=False)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORK HERE!!!!\n",
    "\n",
    "rcParams['figure.figsize'] = 9,9\n",
    "#xlim([-.03, .03])\n",
    "\n",
    "train_classes_relabeled=[] #quick hack to get legend to show Positive/Negative instead of 0/1\n",
    "for i in train_classes:\n",
    "    if i==1:\n",
    "        train_classes_relabeled.append('Positive')\n",
    "    else:\n",
    "        train_classes_relabeled.append('Negative')\n",
    "\n",
    "myplot= sns.stripplot(train_proba_predictions, train_word_list_checked, train_classes_relabeled, jitter=True, size=10)\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=[0, 1],\n",
    "           ncol=2, shadow=True, title=\"True Class\", fancybox=True)\n",
    "\n",
    "plt.axvline(x=0, color='r', linestyle='-')\n",
    "#plt.title('Train Words \\n Positive is feminine/moral/healthy/high-ses \\n Negative is masculine/immoral/unhealthy/low-ses')\n",
    "#plt.xlabel('Predicted')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Testing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Results about Body Weight Langauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_words_seeifinmodel=['obese', 'obesity', 'diabetic', 'diabetes', 'weight', 'overweight', 'thin', 'slender', 'burly',\n",
    "                'muscular', 'diet', 'dieting', 'health', 'healthy', 'unhealthy', 'fat', 'anorexic', 'anorexia', 'bulimia', \n",
    "                'beautiful', 'handsome', 'overeating', 'exercise', 'sedentary', 'bulimic', 'morbidly_obese', 'normal_weight',\n",
    "                'seriously_overweight']\n",
    "obese_word_list= []\n",
    "obese_words=[]\n",
    "\n",
    "#check if these words are in your model\n",
    "for i in range(0, len(obese_words_seeifinmodel)):\n",
    "    try:\n",
    "        currentmodel[obese_words_seeifinmodel[i]]\n",
    "        obese_words.append(currentmodel[obese_words_seeifinmodel[i]])\n",
    "        obese_word_list.append(obese_words_seeifinmodel[i])\n",
    "    except:\n",
    "        print(str(i) + \" was not in this model's vocabulary and has been removed\")\n",
    "        continue\n",
    "obese_words=np.asarray(obese_words)\n",
    "obese_words = preprocessing.normalize(obese_words, norm='l2')\n",
    "obese_predictions =clf.predict(obese_words) #consider using proba rather than binary\n",
    "obese_proba_predictions =clf.predict_proba(obese_words) #consider using proba rather than binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obese 0 [ 0.83567817  0.16432183]\n",
      "obesity 0 [ 0.97957666  0.02042334]\n",
      "diabetic 1 [ 0.29098905  0.70901095]\n",
      "diabetes 0 [ 0.44302918  0.55697082]\n",
      "weight 1 [ 0.24125158  0.75874842]\n",
      "overweight 0 [ 0.93422192  0.06577808]\n",
      "thin 1 [ 0.13589542  0.86410458]\n",
      "slender 1 [  3.24099906e-06   9.99996759e-01]\n",
      "burly 1 [ 0.10080493  0.89919507]\n",
      "muscular 1 [ 0.00970164  0.99029836]\n",
      "diet 1 [ 0.22915466  0.77084534]\n",
      "dieting 0 [ 0.99010841  0.00989159]\n",
      "health 1 [ 0.00821638  0.99178362]\n",
      "healthy 1 [  6.11019209e-07   9.99999389e-01]\n",
      "unhealthy 0 [ 0.99176605  0.00823395]\n",
      "fat 1 [ 0.36354711  0.63645289]\n",
      "anorexic 0 [ 0.98520339  0.01479661]\n",
      "anorexia 0 [ 0.98359532  0.01640468]\n",
      "bulimia 0 [ 0.99771094  0.00228906]\n",
      "beautiful 1 [ 0.00394709  0.99605291]\n",
      "handsome 1 [  6.66233590e-07   9.99999334e-01]\n",
      "overeating 0 [  9.99754328e-01   2.45672244e-04]\n",
      "exercise 1 [ 0.15248789  0.84751211]\n",
      "sedentary 1 [ 0.04108203  0.95891797]\n",
      "bulimic 0 [ 0.98745063  0.01254937]\n",
      "morbidly_obese 1 [ 0.27716831  0.72283169]\n",
      "normal_weight 1 [ 0.00768266  0.99231734]\n",
      "seriously_overweight 1 [ 0.26301341  0.73698659]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(obese_words)):\n",
    "    print(obese_word_list[i], obese_predictions[i], obese_proba_predictions[i])\n",
    "    #print(obese_word_list[i], obese_predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
